{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85c870e",
   "metadata": {},
   "source": [
    "Used the original corpus generator from TA Paul because Kevin's didn't work on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c083dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "#set working directory\n",
    "os.chdir('.\\Corpus_Txt_Files')\n",
    "#function to retreive and turn document into text\n",
    "def retrieve_DSI(file_name):\n",
    "    file_name=str(file_name)\n",
    "    text = docx2txt.process(file_name)\n",
    "    return text\n",
    "#Lists to store file name and body of text\n",
    "file_name=[]\n",
    "text=[]\n",
    "#for loop to iterate through documents in working directory\n",
    "for file in os.listdir('.'):\n",
    "    #if statment to not attempt to open non word documents\n",
    "    if file.endswith('.docx'):\n",
    "        text_name=file\n",
    "        #call function to obtain the text\n",
    "        text_body=retrieve_DSI(file)\n",
    "        #apped the file names and text to list\n",
    "        file_name.append(text_name)\n",
    "        text.append(text_body)\n",
    "        #removed the variables used in the for loop\n",
    "        del text_name, text_body, file\n",
    "#create dictionary for corpus\n",
    "corpus={'DSI_Title':file_name, 'Text': text}\n",
    "#output a CSV with containing the class corpus along with titles of corpus.  \n",
    "#file saved in working directory.\n",
    "pd.DataFrame(corpus).to_csv('Corpus.csv', index=file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4d13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Michael Chyan\\\\Documents\\\\MSDS 453\\\\MSDS453_GroupProject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8595824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a35b8",
   "metadata": {},
   "source": [
    "I got training data from a Kaggle competititon that can be found here: https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset?select=Twitter_Data.csv\n",
    "\n",
    "I chose to only use the Twitter dataset and ignored the Reddit data since it matched up with what Kevin was doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97730e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt completion\n",
       "1   family mormon have never tried explain them t...          1\n",
       "2  buddhism has very much lot compatible with chr...          1\n",
       "3  seriously don say thing first all they won get...         -1\n",
       "4  what you have learned yours and only yours wha...          0\n",
       "5  for your own benefit you may want read living ...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df = pd.read_csv('Reddit_Data.csv', names=['prompt', 'completion'], encoding='ISO-8859-1')\n",
    "# Visualize first 5 rows\n",
    "reddit_df = reddit_df.iloc[1: , :]\n",
    "reddit_df.head()\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368aae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(completion):\n",
    "  if completion == \"0\":\n",
    "    return 'Neutral'\n",
    "  if completion == \"1\":\n",
    "    return 'Positive'\n",
    "  if completion == \"-1\":\n",
    "    return 'Negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c64b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt completion\n",
       "1   family mormon have never tried explain them t...   Positive\n",
       "2  buddhism has very much lot compatible with chr...   Positive\n",
       "3  seriously don say thing first all they won get...   Negative\n",
       "4  what you have learned yours and only yours wha...    Neutral\n",
       "5  for your own benefit you may want read living ...   Positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['completion'] = reddit_df['completion'].apply(convert_labels)\n",
    "\n",
    "reddit_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f57a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion_df.to_csv('cleaned.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03cc2d7",
   "metadata": {},
   "source": [
    "Was planning to fine tune the model using https://beta.openai.com/docs/guides/fine-tuning as a reference but might pivot to using the code block 2 below and hopefully insert in the training data there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97633d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.to_json(\"reddit.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "366f21f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 37249 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- There are 449 duplicated prompt-completion sets. These are rows: [375, 392, 605, 617, 651, 1222, 1528, 1613, 1760, 1835, 1858, 1900, 1975, 2339, 2422, 2446, 2588, 2780, 2796, 2877, 2924, 2928, 3252, 3307, 3420, 3448, 3544, 3550, 3617, 3649, 3939, 3987, 4219, 4292, 4396, 4400, 4415, 4417, 4431, 4453, 4462, 4632, 4786, 5127, 6248, 6778, 6825, 6885, 6952, 7059, 7420, 7685, 7695, 7713, 7857, 8046, 8129, 8179, 8201, 8968, 9014, 9218, 9219, 9297, 9318, 9806, 10258, 10323, 10597, 10618, 10621, 10626, 10630, 10640, 10643, 10662, 10665, 10666, 10678, 10679, 10684, 10699, 10704, 10726, 10929, 11118, 11131, 11311, 11446, 11467, 11484, 11495, 11680, 11825, 11850, 11853, 11943, 11958, 11989, 12254, 12310, 12538, 12570, 12677, 12775, 12777, 12857, 13169, 13296, 13341, 13409, 13573, 13575, 13583, 13831, 13905, 13925, 13931, 13989, 14015, 14123, 14146, 14207, 14216, 14283, 14355, 14359, 14383, 14407, 14460, 14571, 14764, 14770, 14796, 14805, 14806, 14815, 15091, 15141, 15198, 15204, 15385, 15442, 15832, 15940, 15998, 16129, 16276, 16309, 16406, 16537, 16652, 16739, 16880, 16934, 16960, 17068, 17089, 17200, 17272, 17315, 17355, 17359, 17394, 17763, 17817, 17876, 17970, 18101, 18127, 18133, 18163, 18227, 18252, 18342, 18356, 18437, 18483, 18789, 18882, 18889, 18920, 19029, 19069, 19218, 19288, 19336, 19350, 19396, 19454, 19458, 19498, 19598, 19602, 19603, 19694, 19911, 19914, 19924, 19931, 19947, 20020, 20045, 20277, 20321, 20395, 20440, 20448, 20476, 20501, 20610, 20612, 20666, 20669, 20687, 20706, 20753, 20879, 20928, 20987, 21040, 21123, 21179, 21255, 21433, 21656, 21775, 22250, 22316, 22375, 22503, 22571, 22810, 22815, 22877, 22892, 22950, 22971, 23056, 23080, 23084, 23117, 23127, 23618, 23634, 23658, 23776, 23834, 23861, 23862, 23882, 23931, 23995, 24005, 24364, 24557, 24568, 24752, 24786, 24803, 24811, 24816, 24820, 24830, 24857, 25107, 25130, 25155, 25296, 25297, 25333, 25346, 25570, 25630, 25693, 25984, 26056, 26188, 26230, 26254, 26260, 26365, 26786, 26852, 26863, 26864, 26951, 26984, 27032, 27040, 27053, 27102, 27160, 27405, 27432, 27456, 27528, 27529, 27580, 27666, 27797, 27864, 27881, 27922, 27932, 28027, 28030, 28043, 28067, 28073, 28086, 28103, 28162, 28332, 28429, 28786, 28791, 28890, 28972, 29062, 29168, 29346, 29347, 29387, 29400, 29473, 29499, 29502, 29510, 29544, 29578, 29710, 29838, 29844, 29873, 29882, 29899, 29910, 29921, 29931, 30021, 30059, 30319, 30347, 30352, 30404, 30479, 30805, 30967, 31036, 31054, 31370, 31390, 31448, 31538, 31547, 31604, 31748, 31819, 31823, 31859, 32036, 32040, 32261, 32293, 32355, 32374, 32386, 32430, 32474, 32517, 32522, 32532, 32540, 32561, 32791, 32991, 33060, 33300, 33356, 33441, 33502, 33509, 33551, 33585, 33596, 33600, 33799, 33803, 33855, 33887, 33941, 33953, 33956, 33968, 33970, 33982, 33985, 34312, 34376, 34379, 34386, 34415, 34450, 34488, 34620, 34670, 34678, 34719, 34826, 34907, 34915, 34935, 34988, 35003, 35376, 35386, 35429, 35483, 35556, 35644, 35656, 35711, 35741, 35778, 35786, 35943, 35975, 35987, 36036, 36197, 36267, 36523, 36533, 36540, 36661, 36707, 36833, 36835, 36882, 36908, 36915, 37043, 37044, 37111, 37125, 37158, 37234, 37238]\n",
      "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 449 duplicate rows [Y/n]: Y\n",
      "- [Recommended] Add a suffix separator `\\n\\n###\\n\\n` to all prompts [Y/n]: Y\n",
      "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `reddit_prepared_train (5).jsonl` and `reddit_prepared_valid (5).jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"reddit_prepared_train (5).jsonl\" -v \"reddit_prepared_valid (5).jsonl\" --compute_classification_metrics --classification_n_classes 3\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt.\n",
      "Once your model starts training, it'll approximately take 14.76 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Chyan\\anaconda3\\lib\\site-packages\\openai\\validators.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[\"prompt\"] += suffix\n",
      "C:\\Users\\Michael Chyan\\anaconda3\\lib\\site-packages\\openai\\validators.py:421: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[\"completion\"] = x[\"completion\"].apply(\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f reddit.jsonl -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22456ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!openai --api-key \"API_KEY\" api fine_tunes.create -t \"reddit_prepared_train.jsonl\" -v \"reddit_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" Positive\" -m ada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad28952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a69d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf85e719",
   "metadata": {},
   "source": [
    "!openai api fine_tunes.results -i ft-2zaA7qi0rxJduWQpdvOvmGn3 > result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab65fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9273039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6e70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = pd.read_csv('Twitter_Data.csv', names=['prompt', 'completion'], encoding='ISO-8859-1')\n",
    "# Visualize first 5 rows\n",
    "emotion_df = emotion_df.iloc[1: , :]\n",
    "\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df['completion'] = emotion_df['completion'].apply(convert_labels)\n",
    "\n",
    "emotion_df.head()\n",
    "emotion_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df.to_json(\"emotion.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai tools fine_tunes.prepare_data -f emotion.jsonl -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef368d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai --api-key \"API_KEY\" api fine_tunes.create -t \"emotion_prepared_train.jsonl\" -v \"emotion_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" Positive\" -m ada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai --api-key \"API_KEY\" api fine_tunes.follow -i ft-IrhvrED2zAUrgvmdCwwBfmlu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ddb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"API_KEY\"\n",
    "response = openai.Completion.create(\n",
    "engine=\"text-davinci-002\", # this is the most costly model, might switch to curie for similar performance, less cost\n",
    "prompt=\"This is a Sentence sentiment classifier\\n\\n\\nSentence: \\\"I loved the new Batman movie!\\\"\\nSentiment: Positive\\n###\\nSentence: \\\"I hate it when my phone battery dies.\\\"\\nSentiment: Negative\\n###\\nSentence: \\\"My day has been 👍\\\"\\nSentiment: Positive\\n###\\nSentence: \\\"This is the link to the article\\\"\\nSentiment: Neutral\\n###\\nSentence: \\\"This new music video blew my mind\\\"\\nSentiment:\",\n",
    "temperature=0.3,\n",
    "max_tokens=60,\n",
    "top_p=1.0,\n",
    "frequency_penalty=0.5,\n",
    "presence_penalty=0.0,\n",
    "stop=[\"###\"])\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454faa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jsonlines\n",
    "# with jsonlines.open('train.jsonl', mode='w') as writer:\n",
    "#   for row in emotion_df.itertuples():\n",
    "#     writer.write({\n",
    "#       'text': row[1],\n",
    "#       'label': row[2]\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# openai.api_key='API_KEY'\n",
    "# result = openai.File.create(file=open(\"train.jsonl\", errors='ignore'), purpose=\"classifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = result['id']\n",
    "# openai.Classification.create(\n",
    "#   search_model=\"ada\",\n",
    "#   model=\"curie\",\n",
    "#   file=filename,\n",
    "#   query=\"Crypto is crashing hard\",\n",
    "#   labels=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "# )\n",
    "# predicted_label = prediction['label']\n",
    "# print('Predicted label is: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081ee0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923a372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
