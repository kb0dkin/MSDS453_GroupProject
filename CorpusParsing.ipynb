{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and assembling corpus data for the MSDS 453 final project\n",
    "\n",
    "This code will take care of bringing in text files, parsing them into tokens, and store the tokenized version of each text together in a dataframe. We'll also include the vectorized version in the same dataframe, just to make things easier\n",
    "\n",
    "It should also check if a text has already been added, and if it has it will just ignore it.\n",
    "\n",
    "\n",
    "While in the future we could probably build this to run through a webscraper using Selinium or Scrapy, we're just going to have folks select text documents for now.\n",
    "\n",
    "### Start by importing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import pandas as pd # we'll put things into dataframes, then pickle them\n",
    "from os import path\n",
    "\n",
    "from gensim.models import Word2Vec,LdaMulticore, TfidfModel\n",
    "from gensim import corpora\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# allow GUI-based document imports\n",
    "from tkinter import Tk, filedialog\n",
    "import tkinter as tk\n",
    "\n",
    "# os tools\n",
    "from os import path, environ\n",
    "environ['OMP_NUM_THREADS'] = \"1\"\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download stopwords if necessary\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose files, import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9 entries, Announcement2014_CTA to Zotti2011_Reader\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Cleaned_List  9 non-null      object\n",
      " 1   Full_Text     9 non-null      object\n",
      " 2   Publication   9 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "corpus_df = utils.parse_corpus_dir('.')\n",
    "\n",
    "corpus_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF processing\n",
    "\n",
    "tf-idf using sklearn. \n",
    "\n",
    "This is a little more pythonic than the version provided by Paul, but should do all of the same things. Most of the dataframes are named the same, except where I thought a different name might be a little more explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the tfidf vectorizer. ngram range allows you to use multi-words within matrix\n",
    "Tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# fit with final processed documents\n",
    "TFIDF_matrix = Tfidf.fit_transform(corpus_df['Cleaned_List'])\n",
    "TFIDF_df = pd.DataFrame(TFIDF_matrix.toarray(), columns=Tfidf.get_feature_names_out(), index=corpus_df.index)\n",
    "\n",
    "# get some summaries\n",
    "tfidf_mean = np.array(TFIDF_df.mean(axis=0)) # mean tfidf value per term, across documents\n",
    "tfidf_mean_quarts = np.percentile(tfidf_mean, [25, 75]) # quartiles\n",
    "cross_doc_outlier = tfidf_mean_quarts[1] + 1.5*(tfidf_mean_quarts[1]-tfidf_mean_quarts[0])\n",
    "\n",
    "# list of terms that show up frequently\n",
    "outlier_list = TFIDF_df.columns[tfidf_mean>cross_doc_outlier].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = list(corpus_df.index)\n",
    "\n",
    "k= int(np.floor(np.sqrt(len(titles))))\n",
    "km = KMeans(n_clusters=k, random_state =89)\n",
    "km.fit(TFIDF_df)\n",
    "clusters_d2v = km.labels_.tolist()\n",
    "\n",
    "Dictionary={'Doc Name':titles, 'Cluster':clusters_d2v,  'Text': corpus_df['Cleaned_List']}\n",
    "frame=pd.DataFrame(Dictionary, columns=['Cluster', 'Doc Name','Text'])\n",
    "\n",
    "cluster_title = {cluster:titles for cluster,title in enumerate(frame.groupby('Cluster').agg(','.join)['Doc Name'])}\n",
    "\n",
    "\n",
    "k_means_plotting()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now plot the clusters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('MSDS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6f5ab44297089514ffa059ea0d83d6392bebfea7e191fb6b7f26c8412c9553b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
