{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and assembling corpus data for the MSDS 453 final project\n",
    "\n",
    "This code will take care of bringing in text files, parsing them into tokens, and store the tokenized version of each text together in a dataframe. We'll also include the vectorized version in the same dataframe, just to make things easier\n",
    "\n",
    "It should also check if a text has already been added, and if it has it will just ignore it.\n",
    "\n",
    "\n",
    "While in the future we could probably build this to run through a webscraper using Selinium or Scrapy, we're just going to have folks select text documents for now.\n",
    "\n",
    "### Start by importing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import pandas as pd # we'll put things into dataframes, then pickle them\n",
    "from os import path\n",
    "\n",
    "from gensim.models import Word2Vec,LdaMulticore, TfidfModel\n",
    "from gensim import corpora\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# allow GUI-based document imports\n",
    "from tkinter import Tk, filedialog\n",
    "import tkinter as tk\n",
    "\n",
    "# os tools\n",
    "from os import path, environ\n",
    "environ['OMP_NUM_THREADS'] = 1\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download stopwords if necessary\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose files, import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9 entries, Announcement2014_CTA to Zotti2011_Reader\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Cleaned_List  9 non-null      object\n",
      " 1   Full_Text     9 non-null      object\n",
      " 2   Publication   9 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "corpus_df = utils.parse_corpus_dir('.')\n",
    "\n",
    "corpus_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF processing\n",
    "\n",
    "tf-idf using sklearn. \n",
    "\n",
    "This is a little more pythonic than the version provided by Paul, but should do all of the same things. Most of the dataframes are named the same, except where I thought a different name might be a little more explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the tfidf vectorizer. ngram range allows you to use multi-words within matrix\n",
    "Tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# fit with final processed documents\n",
    "TFIDF_matrix = Tfidf.fit_transform(corpus_df['Cleaned_List'])\n",
    "TFIDF_df = pd.DataFrame(TFIDF_matrix.toarray(), columns=Tfidf.get_feature_names_out(), index=corpus_df.index)\n",
    "\n",
    "# get some summaries\n",
    "tfidf_mean = np.array(TFIDF_df.mean(axis=0)) # mean tfidf value per term, across documents\n",
    "tfidf_mean_quarts = np.percentile(tfidf_mean, [25, 75]) # quartiles\n",
    "cross_doc_outlier = tfidf_mean_quarts[1] + 1.5*(tfidf_mean_quarts[1]-tfidf_mean_quarts[0])\n",
    "\n",
    "# list of terms that show up frequently\n",
    "outlier_list = TFIDF_df.columns[tfidf_mean>cross_doc_outlier].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17204\\miniconda3\\envs\\MSDS\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "titles = list(corpus_df.index)\n",
    "\n",
    "k= int(np.floor(np.sqrt(len(titles))))\n",
    "km = KMeans(n_clusters=k, random_state =89)\n",
    "km.fit(TFIDF_df)\n",
    "clusters_d2v = km.labels_.tolist()\n",
    "\n",
    "Dictionary={'Doc Name':titles, 'Cluster':clusters_d2v,  'Text': corpus_df['Cleaned_List']}\n",
    "frame=pd.DataFrame(Dictionary, columns=['Cluster', 'Doc Name','Text'])\n",
    "\n",
    "cluster_title = {cluster:titles for cluster,title in enumerate(frame.groupby('Cluster').agg(','.join)['Doc Name'])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now plot the clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "### Plotting Doc2vec\n",
    "###############################################################################\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "\n",
    "\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "dist = 1 - cosine_similarity(doc2vec_df)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "\n",
    "#set up colors per clusters using a dict.  number of colors must correspond to K\n",
    "cluster_colors = {0: 'black', 1: 'grey', 2: 'blue', 3: 'rosybrown', 4: 'firebrick', \n",
    "                  5:'red', 6:'darksalmon', 7:'sienna'}\n",
    "\n",
    "\n",
    "#set up cluster names using a dict.  \n",
    "cluster_dict=cluster_title         \n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters_d2v, title=range(0,len(clusters_d2v)))) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "fig, ax = plt.subplots() # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12,\n",
    "            label=cluster_dict[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    # ax.set_aspect('auto')\n",
    "    # ax.tick_params(\\\n",
    "    #     axis= 'x',          # changes apply to the x-axis\n",
    "    #     which='both',      # both major and minor ticks are affected\n",
    "    #     bottom='off',      # ticks along the bottom edge are off\n",
    "    #     top='off',         # ticks along the top edge are off\n",
    "    #     labelbottom='on')\n",
    "    # ax.tick_params(\\\n",
    "    #     axis= 'y',         # changes apply to the y-axis\n",
    "    #     which='both',      # both major and minor ticks are affected\n",
    "    #     left='off',      # ticks along the bottom edge are off\n",
    "    #     right='off',         # ticks along the top edge are off\n",
    "    #     labelleft='on')\n",
    "\n",
    "for spine in ax.spines:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,0.5))      #show legend with only 1 point\n",
    "\n",
    "fig.set_size_inches([9,9])\n",
    "\n",
    "fig.savefig('Doc2Vec.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.pathsep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.sep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('MSDS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6f5ab44297089514ffa059ea0d83d6392bebfea7e191fb6b7f26c8412c9553b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
